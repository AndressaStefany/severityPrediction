# How to fit a lama on multiple gpus


## Research links

### Huggingface

https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.GPTQConfig

### Using external tools
https://github.com/microsoft/DeepSpeed/issues/4145

### Estimation of VRAM required
https://github.com/facebookresearch/llama-recipes/issues/172
https://www.reddit.com/r/LocalLLaMA/comments/15rlqsb/how_to_perform_multigpu_parallel_inference_for/