{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity prediction of bug reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess: \n",
    "- Embedding using:\n",
    "- Algorithms for binary classification:\n",
    "- Metrics: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # For text data\n",
    "from sklearn.svm import SVC  # Or other classification algorithm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from baseline_functions import filter_bug_severity, create_binary_feature, remove_urls_and_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 361006 entries, 0 to 361005\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   _id           361006 non-null  object\n",
      " 1   bug_id        361006 non-null  int64 \n",
      " 2   product       361006 non-null  object\n",
      " 3   description   361006 non-null  object\n",
      " 4   bug_severity  361006 non-null  object\n",
      " 5   dup_id        361006 non-null  object\n",
      " 6   short_desc    361006 non-null  object\n",
      " 7   priority      361006 non-null  object\n",
      " 8   version       361006 non-null  object\n",
      " 9   component     361006 non-null  object\n",
      " 10  delta_ts      361006 non-null  object\n",
      " 11  bug_status    361006 non-null  object\n",
      " 12  creation_ts   361006 non-null  object\n",
      " 13  resolution    361006 non-null  object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 38.6+ MB\n"
     ]
    }
   ],
   "source": [
    "bug_reports = pd.read_json('data/eclipse_clear.json', lines=True)\n",
    "bug_reports.info()\n",
    "############ fazer um script para pegar o summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(dataframe, col='description'):\n",
    "    bug_reports_copy = dataframe.copy()\n",
    "    \n",
    "    tokens = bug_reports_copy[col].apply(word_tokenize)\n",
    "    #print(tokens)\n",
    "    \n",
    "    # Get the set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Define the special characters to remove\n",
    "    # in the article doesn't include this step\n",
    "    special_characters = set(string.punctuation)\n",
    "    special_characters.add('``')\n",
    "    special_characters.add(\"''\")\n",
    "    # there is also:  \"n't\" / checkar\n",
    "    \n",
    "    # Initialize the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Apply stop-word removal and stemming\n",
    "    filtered_texts = []\n",
    "    for tokens in tokens:\n",
    "        filtered_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words and token not in special_characters]\n",
    "        # filtered_texts.append(filtered_tokens)\n",
    "        filtered_texts.append(' '.join(filtered_tokens))\n",
    "    #print('\\nfiltered_texts', filtered_texts)\n",
    "    bug_reports_copy['preprocess_desc'] = filtered_texts\n",
    "    \n",
    "    return bug_reports_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361006"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_reports.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checar se estou tratando os enters\n",
    "# qual a melhor ordem de colocar o tratamento dos enters\n",
    "# testar o passo de remover URL e CODE\n",
    "# checar a estrutura da primeira referencia\n",
    "# fazer os testes em pedaÃ§os dos bugs e ir salvando os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocess_text(bug_reports, col='description')# adicionar o tratamento do campo vazio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preprocess_text(bug_reports, col='description')\n",
    "test.to_csv('my_dataframe.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19840,bug_reports.shape[0]):\n",
    "    print(i,'teste')\n",
    "    preprocess_text(bug_reports[i-9:i], col='description')\n",
    "    #if i > 10:\n",
    "    #    preprocess_text(bug_reports[i-9:i], col='description')\n",
    "    #else:\n",
    "    #    preprocess_text(bug_reports[:i], col='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bug_severity_filter', FunctionTransformer(filter_bug_severity, kw_args={'col': 'bug_severity'})),\n",
    "    ('binary_feature', FunctionTransformer(create_binary_feature, kw_args={'col': 'bug_severity'})),\n",
    "    ('remove_url_and_code', , FunctionTransformer(remove_urls_and_codes, kw_args={'col': 'bug_severity'})),\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text)),\n",
    "    # ('embedding', embedding),\n",
    "    # ('classifier', classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First step\n",
      "Columns: Index(['_id', 'bug_id', 'description', 'bug_severity'], dtype='object')\n",
      "Return type: <class 'pandas.core.frame.DataFrame'>\n",
      "Severities's kind: ['major' 'critical' 'minor' 'trivial' 'blocker'] \n",
      "\n",
      "Second step\n",
      "Columns: Index(['_id', 'bug_id', 'description', 'bug_severity', 'binary_severity'], dtype='object')\n",
      "Return type: <class 'pandas.core.frame.DataFrame'>\n",
      "Severities's kind: [1 0] \n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSeverities\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms kind:\u001b[39m\u001b[39m\"\u001b[39m, binary_feature_result\u001b[39m.\u001b[39mbinary_severity\u001b[39m.\u001b[39munique(), \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Get the transformed data at the preprocessor\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m preprocessor_result \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mnamed_steps[\u001b[39m'\u001b[39;49m\u001b[39mpreprocessor\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mtransform(binary_feature_result)\n\u001b[0;32m     19\u001b[0m \u001b[39m#print('Third step')\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m#print('Columns:', preprocessor_result.columns)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m#print('Return type:', type(preprocessor_result))\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m#print(\"Severities's kind:\", preprocessor_result.head(3).preprocess_desc, '\\n')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dessa\\anaconda3\\envs\\Master\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\dessa\\anaconda3\\envs\\Master\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:240\u001b[0m, in \u001b[0;36mFunctionTransformer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transform X using the forward function.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \n\u001b[0;32m    228\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m    Transformed input.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_input(X, reset\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(X, func\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, kw_args\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkw_args)\n",
      "File \u001b[1;32mc:\\Users\\dessa\\anaconda3\\envs\\Master\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:312\u001b[0m, in \u001b[0;36mFunctionTransformer._transform\u001b[1;34m(self, X, func, kw_args)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    310\u001b[0m     func \u001b[39m=\u001b[39m _identity\n\u001b[1;32m--> 312\u001b[0m \u001b[39mreturn\u001b[39;00m func(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(kw_args \u001b[39mif\u001b[39;00m kw_args \u001b[39melse\u001b[39;00m {}))\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(dataframe, col)\u001b[0m\n\u001b[0;32m      2\u001b[0m bug_reports_copy \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[39m# Remove URLs using regular expressions\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m bug_reports_copy[col] \u001b[39m=\u001b[39m bug_reports_copy[col]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m text: re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mS+\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, text))\n\u001b[0;32m      7\u001b[0m \u001b[39m# Remove programming code snippets using regular expressions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m bug_reports_copy[col] \u001b[39m=\u001b[39m bug_reports_copy[col]\u001b[39m.\u001b[39mapply(remove_code_snippets)\n",
      "File \u001b[1;32mc:\\Users\\dessa\\anaconda3\\envs\\Master\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\dessa\\anaconda3\\envs\\Master\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\dessa\\anaconda3\\envs\\Master\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\dessa\\anaconda3\\envs\\Master\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m, in \u001b[0;36mpreprocess_text.<locals>.<lambda>\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m bug_reports_copy \u001b[39m=\u001b[39m dataframe\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m      4\u001b[0m \u001b[39m# Remove URLs using regular expressions\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m bug_reports_copy[col] \u001b[39m=\u001b[39m bug_reports_copy[col]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m text: re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhttp\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mS+\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, text))\n\u001b[0;32m      7\u001b[0m \u001b[39m# Remove programming code snippets using regular expressions\u001b[39;00m\n\u001b[0;32m      8\u001b[0m bug_reports_copy[col] \u001b[39m=\u001b[39m bug_reports_copy[col]\u001b[39m.\u001b[39mapply(remove_code_snippets)\n",
      "File \u001b[1;32mc:\\Users\\dessa\\anaconda3\\envs\\Master\\lib\\re.py:210\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msub\u001b[39m(pattern, repl, string, count\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, flags\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m    204\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \u001b[39m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mreturn\u001b[39;00m _compile(pattern, flags)\u001b[39m.\u001b[39;49msub(repl, string, count)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "pipeline.fit(bug_reports)\n",
    "\n",
    "# Get the transformed data at the filter_bug_severity step\n",
    "filter_bug_severity_result = pipeline.named_steps['bug_severity_filter'].transform(bug_reports) # ver sobre a seleÃ§Ã£o das colunas\n",
    "print('First step')\n",
    "print('Columns:', filter_bug_severity_result.columns)\n",
    "print('Return type:', type(filter_bug_severity_result))\n",
    "print(\"Severities's kind:\", filter_bug_severity_result.bug_severity.unique(), '\\n')\n",
    "\n",
    "# Get the transformed data at the binary_feature step\n",
    "binary_feature_result = pipeline.named_steps['binary_feature'].transform(filter_bug_severity_result)\n",
    "print('Second step')\n",
    "print('Columns:', binary_feature_result.columns)\n",
    "print('Return type:', type(binary_feature_result))\n",
    "print(\"Severities's kind:\", binary_feature_result.binary_severity.unique(), '\\n')\n",
    "\n",
    "# Get the transformed data at the preprocessor\n",
    "preprocessor_result = pipeline.named_steps['preprocessor'].transform(binary_feature_result)\n",
    "#print('Third step')\n",
    "#print('Columns:', preprocessor_result.columns)\n",
    "#print('Return type:', type(preprocessor_result))\n",
    "#print(\"Severities's kind:\", preprocessor_result.head(3).preprocess_desc, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline for production\n",
    "production_pipeline = Pipeline([\n",
    "    ('bug_severity_filter', FunctionTransformer(filter_bug_severity, kw_args={'col': 'bug_severity'})),\n",
    "    ('binary_feature', FunctionTransformer(create_binary_feature, kw_args={'col': 'bug_severity'})),\n",
    "    ('preprocessing', TextPreprocessor()),\n",
    "    ('embedding', Word2VecEmbedding()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Fit the production pipeline on the entire dataset\n",
    "production_pipeline.fit(X_text, y)  # Assuming X_text and y are already defined\n",
    "\n",
    "# Make predictions on new, unseen data\n",
    "new_data = ['New sentence 1.', 'New sentence 2.', ...]\n",
    "predictions = production_pipeline.predict(new_data)\n",
    "print(predictions)\n",
    "In this production pipeline, you've removed the data splitting step and fitted the pipeline on the entire dataset. Then, you can use this pipeline to make predictions on new, unseen data by calling the predict method with the new data. This is a common approach in production when you're deploying a trained model for making predictions on real-world data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
