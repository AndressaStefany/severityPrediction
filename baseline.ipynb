{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severity prediction of bug reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocess: \n",
    "- Embedding using:\n",
    "- Algorithms for binary classification:\n",
    "- Metrics: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # For text data\n",
    "from sklearn.svm import SVC  # Or other classification algorithm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 361006 entries, 0 to 361005\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   _id           361006 non-null  object\n",
      " 1   bug_id        361006 non-null  int64 \n",
      " 2   product       361006 non-null  object\n",
      " 3   description   361006 non-null  object\n",
      " 4   bug_severity  361006 non-null  object\n",
      " 5   dup_id        361006 non-null  object\n",
      " 6   short_desc    361006 non-null  object\n",
      " 7   priority      361006 non-null  object\n",
      " 8   version       361006 non-null  object\n",
      " 9   component     361006 non-null  object\n",
      " 10  delta_ts      361006 non-null  object\n",
      " 11  bug_status    361006 non-null  object\n",
      " 12  creation_ts   361006 non-null  object\n",
      " 13  resolution    361006 non-null  object\n",
      "dtypes: int64(1), object(13)\n",
      "memory usage: 38.6+ MB\n"
     ]
    }
   ],
   "source": [
    "bug_reports = pd.read_json('data/eclipse_clear.json', lines=True)\n",
    "bug_reports.info()\n",
    "############ fazer um script para pegar o summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bug_severity(bug_reports, col='bug_severity'):\n",
    "    # precisa filtrar a resolusao/bug_status ????\n",
    "    filtered_reports = bug_reports[~bug_reports[col].isin(['normal', 'enhancement'])]\n",
    "    selected_columns = ['_id', 'bug_id', 'description', 'bug_severity']\n",
    "    return filtered_reports[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_feature(bug_reports, col='bug_severity'):\n",
    "    def binary_feature_creator(severity):\n",
    "        return 1 if severity in ['blocker', 'critical', 'major'] else 0\n",
    "    \n",
    "    bug_reports_copy = bug_reports.copy()\n",
    "    bug_reports_copy['binary_severity'] = bug_reports_copy['bug_severity'].apply(binary_feature_creator)\n",
    "    return bug_reports_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver como remover sem sinalizador, e como está no dado\n",
    "def remove_code_snippets(text):\n",
    "    # Remove programming code snippets enclosed in triple backticks\n",
    "    code_pattern = r'```(?:[^`]+|`(?!``))*```'\n",
    "    text_without_code = re.sub(code_pattern, '', text)\n",
    "    \n",
    "    # Remove programming code snippets enclosed in single backticks\n",
    "    text_without_code = re.sub(r'`[^`]+`', '', text_without_code)\n",
    "    \n",
    "    return text_without_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(dataframe, col='description'): #changing the inputs\n",
    "    bug_reports_copy = dataframe.copy()\n",
    "    \n",
    "    # Remove URLs using regular expressions\n",
    "    bug_reports_copy[col] = bug_reports_copy[col].apply(lambda text: re.sub(r'http\\S+', '', text))\n",
    "    \n",
    "    # Remove programming code snippets using regular expressions\n",
    "    bug_reports_copy[col] = bug_reports_copy[col].apply(remove_code_snippets)\n",
    "    \n",
    "    tokens = bug_reports_copy[col].apply(word_tokenize)\n",
    "    print(tokens)\n",
    "    \n",
    "    # Get the set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Define the special characters to remove\n",
    "    # in the article doesn't include this step\n",
    "    special_characters = set(string.punctuation)\n",
    "    special_characters.add('``')\n",
    "    special_characters.add(\"''\")\n",
    "    # there is also:  \"n't\" / checkar\n",
    "    \n",
    "    # Initialize the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Apply stop-word removal and stemming\n",
    "    filtered_texts = []\n",
    "    for tokens in tokens:\n",
    "        filtered_tokens = [stemmer.stem(token) for token in tokens if token.lower() not in stop_words and token not in special_characters]\n",
    "        # filtered_texts.append(filtered_tokens)\n",
    "        filtered_texts.append(' '.join(filtered_tokens))\n",
    "    print('\\nfiltered_texts', filtered_texts)\n",
    "    bug_reports_copy['preprocess_desc'] = filtered_texts\n",
    "    \n",
    "    return bug_reports_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bug_severity_filter', FunctionTransformer(filter_bug_severity, kw_args={'col': 'bug_severity'})),\n",
    "    ('binary_feature', FunctionTransformer(create_binary_feature, kw_args={'col': 'bug_severity'})),\n",
    "    ('preprocessor', FunctionTransformer(preprocess_text)),\n",
    "    # ('embedding', embedding),\n",
    "    # ('classifier', classifier)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First step\n",
      "Columns: Index(['_id', 'bug_id', 'description', 'bug_severity'], dtype='object')\n",
      "Return type: <class 'pandas.core.frame.DataFrame'>\n",
      "Severities's kind: ['major' 'critical' 'minor' 'trivial' 'blocker'] \n",
      "\n",
      "Second step\n",
      "Columns: Index(['_id', 'bug_id', 'description', 'bug_severity', 'binary_severity'], dtype='object')\n",
      "Return type: <class 'pandas.core.frame.DataFrame'>\n",
      "Severities's kind: [1 0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(bug_reports)\n",
    "\n",
    "# Get the transformed data at the filter_bug_severity step\n",
    "filter_bug_severity_result = pipeline.named_steps['bug_severity_filter'].transform(bug_reports) # ver sobre a seleção das colunas\n",
    "print('First step')\n",
    "print('Columns:', filter_bug_severity_result.columns)\n",
    "print('Return type:', type(filter_bug_severity_result))\n",
    "print(\"Severities's kind:\", filter_bug_severity_result.bug_severity.unique(), '\\n')\n",
    "\n",
    "# Get the transformed data at the binary_feature step\n",
    "binary_feature_result = pipeline.named_steps['binary_feature'].transform(filter_bug_severity_result)\n",
    "print('Second step')\n",
    "print('Columns:', binary_feature_result.columns)\n",
    "print('Return type:', type(binary_feature_result))\n",
    "print(\"Severities's kind:\", binary_feature_result.binary_severity.unique(), '\\n')\n",
    "\n",
    "# Get the transformed data at the preprocessor\n",
    "#preprocessor_result = pipeline.named_steps['preprocessor'].transform(binary_feature_result)\n",
    "#print('Third step')\n",
    "#print('Columns:', preprocessor_result.columns)\n",
    "#print('Return type:', type(preprocessor_result))\n",
    "#print(\"Severities's kind:\", preprocessor_result.head(3).preprocess_desc, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline for production\n",
    "production_pipeline = Pipeline([\n",
    "    ('bug_severity_filter', FunctionTransformer(filter_bug_severity, kw_args={'col': 'bug_severity'})),\n",
    "    ('binary_feature', FunctionTransformer(create_binary_feature, kw_args={'col': 'bug_severity'})),\n",
    "    ('preprocessing', TextPreprocessor()),\n",
    "    ('embedding', Word2VecEmbedding()),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Fit the production pipeline on the entire dataset\n",
    "production_pipeline.fit(X_text, y)  # Assuming X_text and y are already defined\n",
    "\n",
    "# Make predictions on new, unseen data\n",
    "new_data = ['New sentence 1.', 'New sentence 2.', ...]\n",
    "predictions = production_pipeline.predict(new_data)\n",
    "print(predictions)\n",
    "In this production pipeline, you've removed the data splitting step and fitted the pipeline on the entire dataset. Then, you can use this pipeline to make predictions on new, unseen data by calling the predict method with the new data. This is a common approach in production when you're deploying a trained model for making predictions on real-world data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
