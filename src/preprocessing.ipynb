{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'explore.truncate_and_template' from 'c:\\\\Users\\\\robin\\\\Documents\\\\projets\\\\severityPrediction\\\\src\\\\explore\\\\truncate_and_template.py'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from importlib import reload\n",
    "sys.path.append(\".\")\n",
    "import baseline.baseline_functions as bl\n",
    "import explore.truncate_and_template as tmp\n",
    "reload(bl)\n",
    "reload(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(\"../data/llm/data_preprocessed_tokens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant fields from the json file predictions_v100l.json\n",
    "df = pd.read_json(path_data)\n",
    "fields = [\"bug_id\",\"binary_severity\",\"text\",\"tokenized_text\"]\n",
    "df = df[fields]\n",
    "df.rename({\"text\":\"nltk_description\",\"tokenized_text\":\"llama_tokenized_description\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "1it [00:00, 31.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing {'description': 'Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Input:\\n\\n\\n### Remembering the instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Response:'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the template and apply preprocessing to get nltk template and llama2 tokenized version\n",
    "template = tmp.get_template()\n",
    "df_template = pd.DataFrame([{\"description\":template}])\n",
    "nltk_template = bl.preprocess_text(df_template).iloc[0].description\n",
    "data = [{\"description\":nltk_template}]\n",
    "tmp.truncate_and_transform(data)\n",
    "llama_tokenized_description = data[0][\"truncated_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Categorize the bug report into one of 2 categories:\n",
      "\n",
      "0 = NOT SEVERE\n",
      "1 = SEVERE\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Remembering the instruction:\n",
      "Categorize the bug report into one of 2 categories:\n",
      "\n",
      "0 = NOT SEVERE\n",
      "1 = SEVERE\n",
      "\n",
      "### Response:\n",
      "****************************************************************************************************\n",
      "['<0x0A>', '<0x0A>', '<0x0A>', '##', '#', '▁Remember', 'ing', '▁the', '▁instruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Response', ':']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_template)\n",
    "print(\"*\"*100)\n",
    "print(llama_tokenized_description[101:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the final json\n",
    "json_dict = {\n",
    "    \"template\":{\n",
    "        \"nltk_template\":nltk_template,\n",
    "        \"llama_tokenized_description\":llama_tokenized_description,\n",
    "        \"description_index_insert\": 101\n",
    "    },\n",
    "    \"data\":df.to_dict(orient=\"records\")\n",
    "}\n",
    "with open(\"../data/llm/data_preprocessed_tokens_v2.json\",\"w\") as f:\n",
    "    json.dump(json_dict,f,indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "severityPrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
