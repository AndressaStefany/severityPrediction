{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\robin\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from importlib import reload\n",
    "sys.path.append(\".\")\n",
    "import baseline.baseline_functions as bl\n",
    "import explore.truncate_and_template as tmp\n",
    "reload(bl)\n",
    "reload(tmp)\n",
    "from huggingface_hub import login\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(\"../data/llm/data_preprocessed_tokens.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant fields from the json file predictions_v100l.json\n",
    "df = pd.read_json(path_data)\n",
    "fields = [\"bug_id\",\"binary_severity\",\"text\",\"tokenized_text\"]\n",
    "df = df[fields]\n",
    "df.rename({\"text\":\"nltk_description\",\"tokenized_text\":\"llama_tokenized_description\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing {'description': 'Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Input:\\n\\n\\n### Remembering the instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Response:'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the template and apply preprocessing to get nltk template and llama2 tokenized version\n",
    "template = tmp.get_template()\n",
    "df_template = pd.DataFrame([{\"description\":template}])\n",
    "nltk_template = bl.preprocess_text(df_template).iloc[0].description\n",
    "data = [{\"description\":nltk_template}]\n",
    "tmp.truncate_and_transform(data)\n",
    "llama_tokenized_description = data[0][\"truncated_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Categorize the bug report into one of 2 categories:\n",
      "\n",
      "0 = NOT SEVERE\n",
      "1 = SEVERE\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Remembering the instruction:\n",
      "Categorize the bug report into one of 2 categories:\n",
      "\n",
      "0 = NOT SEVERE\n",
      "1 = SEVERE\n",
      "\n",
      "### Response:\n",
      "****************************************************************************************************\n",
      "['<0x0A>', '<0x0A>', '<0x0A>', '##', '#', '▁Remember', 'ing', '▁the', '▁instruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Response', ':']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_template)\n",
    "print(\"*\"*100)\n",
    "print(llama_tokenized_description[101:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the final json\n",
    "json_dict = {\n",
    "    \"template\":{\n",
    "        \"nltk_template\":nltk_template,\n",
    "        \"llama_tokenized_template\":llama_tokenized_description,\n",
    "        \"template_index_insert\": 101\n",
    "    },\n",
    "    \"data\":df.to_dict(orient=\"records\")\n",
    "}\n",
    "with open(\"../data/llm/data_preprocessed_tokens_v2.json\",\"w\") as f:\n",
    "    json.dump(json_dict,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "# Set the padding token if it's not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Input: project junk releas teamstream renam project attempt releas teamstream compar fail junk exist project must renam back old name artifact left first renam left must delet project releas project releas show conflict element compar phase note km 5/22/01 9:42:15 pm need flush sync info renam jean-michel 31/05/2001 6:35:07 pm time-permit\\n\\n\\n### Remembering the instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Response:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of training code\n",
    "template = json_dict[\"template\"]\n",
    "d = json_dict[\"data\"][0]\n",
    "# we put the template and the description together using the insertion point specified in description_index_insert\n",
    "## First make a copy\n",
    "tokenized_full_text = template[\"llama_tokenized_template\"][:]\n",
    "## Then remove the <s> token from the description\n",
    "description = d[\"llama_tokenized_description\"][1:]\n",
    "## Then insert the description inside the template at the position indicated (after input)\n",
    "template_index_insert = template[\"template_index_insert\"]\n",
    "tokenized_full_text[template_index_insert:template_index_insert] = description\n",
    "tokenized_full_text.pop(0)\n",
    "tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenized_full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "1it [00:00, 34.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing {'description': '▁Always ▁answer ▁with ▁one ▁token . ▁Do ▁not ▁give ▁any ▁explanation . ▁Use ▁only ▁ 0 ▁or ▁ 1 ▁and ▁one ▁token . ▁Sk ip ▁any ▁polit eness ▁answer . ▁You ▁have ▁only ▁one ▁word ▁available . <0x0A> Bel ow ▁is ▁an ▁instruction ▁that ▁describes ▁a ▁task . ▁Write ▁a ▁response ▁that ▁appropri ately ▁comple tes ▁the ▁request . <0x0A> <0x0A> ## # ▁Inst ruction : <0x0A> C ategor ize ▁the ▁bug ▁report ▁into ▁one ▁of ▁ 2 ▁categories : <0x0A> <0x0A> 0 ▁= ▁NOT ▁SE VER E <0x0A> 1 ▁= ▁SE VER E <0x0A> <0x0A> ## # ▁Input : ▁project ▁j unk ▁re le as ▁team stream ▁ren am ▁project ▁attempt ▁re le as ▁team stream ▁compar ▁fail ▁j unk ▁exist ▁project ▁must ▁ren am ▁back ▁old ▁name ▁artifact ▁left ▁first ▁ren am ▁left ▁must ▁delet ▁project ▁re le as ▁project ▁re le as ▁show ▁conflict ▁element ▁compar ▁phase ▁note ▁km ▁ 5 / 2 2 / 0 1 ▁ 9 : 4 2 : 1 5 ▁pm ▁need ▁flush ▁sync ▁info ▁ren am ▁je an - m ich el ▁ 3 1 / 0 5 / 2 0 0 1 ▁ 6 : 3 5 : 0 7 ▁pm ▁time - per mit <0x0A> <0x0A> <0x0A> ## # ▁Remember ing ▁the ▁instruction : <0x0A> C ategor ize ▁the ▁bug ▁report ▁into ▁one ▁of ▁ 2 ▁categories : <0x0A> <0x0A> 0 ▁= ▁NOT ▁SE VER E <0x0A> 1 ▁= ▁SE VER E <0x0A> <0x0A> ## # ▁Response :'}\n",
      "****************************************************************************************************\n",
      "['<s>', '▁project', '▁j', 'unk', '▁re', 'le', 'as', '▁team', 'stream', '▁ren', 'am', '▁project', '▁attempt', '▁re', 'le', 'as', '▁team', 'stream', '▁compar', '▁fail', '▁j', 'unk', '▁exist', '▁project', '▁must', '▁ren', 'am', '▁back', '▁old', '▁name', '▁artifact', '▁left', '▁first', '▁ren', 'am', '▁left', '▁must', '▁delet', '▁project', '▁re', 'le', 'as', '▁project', '▁re', 'le', 'as', '▁show', '▁conflict', '▁element', '▁compar', '▁phase', '▁note', '▁km', '▁', '5', '/', '2', '2', '/', '0', '1', '▁', '9', ':', '4', '2', ':', '1', '5', '▁pm', '▁need', '▁flush', '▁sync', '▁info', '▁ren', 'am', '▁je', 'an', '-', 'm', 'ich', 'el', '▁', '3', '1', '/', '0', '5', '/', '2', '0', '0', '1', '▁', '6', ':', '3', '5', ':', '0', '7', '▁pm', '▁time', '-', 'per', 'mit']\n",
      "****************************************************************************************************\n",
      "['<s>', '▁Always', '▁answer', '▁with', '▁one', '▁token', '.', '▁Do', '▁not', '▁give', '▁any', '▁explanation', '.', '▁Use', '▁only', '▁', '0', '▁or', '▁', '1', '▁and', '▁one', '▁token', '.', '▁Sk', 'ip', '▁any', '▁polit', 'eness', '▁answer', '.', '▁You', '▁have', '▁only', '▁one', '▁word', '▁available', '.', '<0x0A>', 'Bel', 'ow', '▁is', '▁an', '▁instruction', '▁that', '▁describes', '▁a', '▁task', '.', '▁Write', '▁a', '▁response', '▁that', '▁appropri', 'ately', '▁comple', 'tes', '▁the', '▁request', '.', '<0x0A>', '<0x0A>', '##', '#', '▁Inst', 'ruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Input', ':', '<0x0A>', '<0x0A>', '<0x0A>', '##', '#', '▁Remember', 'ing', '▁the', '▁instruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Response', ':']\n",
      "****************************************************************************************************\n",
      "['<s>', '▁', '▁Always', '▁', '▁answer', '▁', '▁with', '▁', '▁one', '▁', '▁token', '▁.', '▁', '▁Do', '▁', '▁not', '▁', '▁give', '▁', '▁any', '▁', '▁explanation', '▁.', '▁', '▁Use', '▁', '▁only', '▁▁▁', '0', '▁', '▁or', '▁▁▁', '1', '▁', '▁and', '▁', '▁one', '▁', '▁token', '▁.', '▁', '▁Sk', '▁ip', '▁', '▁any', '▁', '▁polit', '▁en', 'ess', '▁', '▁answer', '▁.', '▁', '▁You', '▁', '▁have', '▁', '▁only', '▁', '▁one', '▁', '▁word', '▁', '▁available', '▁.', '▁<', '0', 'x', '0', 'A', '>', '▁Bel', '▁ow', '▁', '▁is', '▁', '▁an', '▁', '▁instruction', '▁', '▁that', '▁', '▁describes', '▁', '▁a', '▁', '▁task', '▁.', '▁', '▁Write', '▁', '▁a', '▁', '▁response', '▁', '▁that', '▁', '▁appropri', '▁at', 'ely', '▁', '▁comple', '▁t', 'es', '▁', '▁the', '▁', '▁request', '▁.', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁##', '▁#', '▁', '▁Inst', '▁ru', 'ction', '▁:', '▁<', '0', 'x', '0', 'A', '>', '▁C', '▁a', 'te', 'gor', '▁', 'ize', '▁', '▁the', '▁', '▁bug', '▁', '▁report', '▁', '▁into', '▁', '▁one', '▁', '▁of', '▁▁▁', '2', '▁', '▁categories', '▁:', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁', '0', '▁', '▁=', '▁', '▁NOT', '▁', '▁SE', '▁V', 'ER', '▁E', '▁<', '0', 'x', '0', 'A', '>', '▁', '1', '▁', '▁=', '▁', '▁SE', '▁V', 'ER', '▁E', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁##', '▁#', '▁', '▁Input', '▁:', '▁', '▁project', '▁', '▁j', '▁un', 'k', '▁', '▁re', '▁le', '▁as', '▁', '▁team', '▁stream', '▁', '▁ren', '▁am', '▁', '▁project', '▁', '▁attempt', '▁', '▁re', '▁le', '▁as', '▁', '▁team', '▁stream', '▁', '▁compar', '▁', '▁fail', '▁', '▁j', '▁un', 'k', '▁', '▁exist', '▁', '▁project', '▁', '▁must', '▁', '▁ren', '▁am', '▁', '▁back', '▁', '▁old', '▁', '▁name', '▁', '▁artifact', '▁', '▁left', '▁', '▁first', '▁', '▁ren', '▁am', '▁', '▁left', '▁', '▁must', '▁', '▁delet', '▁', '▁project', '▁', '▁re', '▁le', '▁as', '▁', '▁project', '▁', '▁re', '▁le', '▁as', '▁', '▁show', '▁', '▁conflict', '▁', '▁element', '▁', '▁compar', '▁', '▁phase', '▁', '▁note', '▁', '▁km', '▁▁▁', '5', '▁/', '▁', '2', '▁', '2', '▁/', '▁', '0', '▁', '1', '▁▁▁', '9', '▁:', '▁', '4', '▁', '2', '▁:', '▁', '1', '▁', '5', '▁', '▁pm', '▁', '▁need', '▁', '▁flush', '▁', '▁sync', '▁', '▁info', '▁', '▁ren', '▁am', '▁', '▁je', '▁an', '▁-', '▁m', '▁ich', '▁el', '▁▁▁', '3', '▁', '1', '▁/', '▁', '0', '▁', '5', '▁/', '▁', '2', '▁', '0', '▁', '0', '▁', '1', '▁▁▁', '6', '▁:', '▁', '3', '▁', '5', '▁:', '▁', '0', '▁', '7', '▁', '▁pm', '▁', '▁time', '▁-', '▁per', '▁mit', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁##', '▁#', '▁', '▁Remember', '▁ing', '▁', '▁the', '▁', '▁instruction', '▁:', '▁<', '0', 'x', '0', 'A', '>', '▁C', '▁a', 'te', 'gor', '▁', 'ize', '▁', '▁the', '▁', '▁bug', '▁', '▁report', '▁', '▁into', '▁', '▁one', '▁', '▁of', '▁▁▁', '2', '▁', '▁categories', '▁:', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁', '0', '▁', '▁=', '▁', '▁NOT', '▁', '▁SE', '▁V', 'ER', '▁E', '▁<', '0', 'x', '0', 'A', '>', '▁', '1', '▁', '▁=', '▁', '▁SE', '▁V', 'ER', '▁E', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁##', '▁#', '▁', '▁Response', '▁:']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{\"description\":\" \".join(tokenized_full_text)}]\n",
    "tmp.truncate_and_transform(data)\n",
    "print(\"*\"*100)\n",
    "print(d[\"llama_tokenized_description\"])\n",
    "print(\"*\"*100)\n",
    "print(template[\"llama_tokenized_template\"])\n",
    "print(\"*\"*100)\n",
    "print(data[0][\"truncated_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "severityPrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
