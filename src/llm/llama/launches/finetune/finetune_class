#!/bin/bash
#SBATCH --nodes=1
#SBATCH --gpus-per-node=v100l:1 # on cedar choose one v100l gpu but can be also p100l
#SBATCH --cpus-per-task=1 # change this parameter to 2,4,6,... and increase "--num_workers" accordingly to see the effect on performance
#SBATCH --time=00-01:00:00           # duration (JJ-HH:MM:SS)
#SBATCH --output=log-%x-%j.out
#SBATCH --error=log-%x-%j.err
#SBATCH --mail-user=robin.moine456@gmail.com
#SBATCH --mail-type=ALL
#SBATCH --mem=64G # Request 8 GB of RAM

# syntax of 
# Capture the start time
start_time=$(date +"%Y-%m-%d %H:%M:%S")

# loading modules

module load StdEnv/2020  gcc/9.3.0  cuda/11.4 python/3.11 arrow ## cuda/11.7 not work
virtualenv $SLURM_TMPDIR/MYENV
source $SLURM_TMPDIR/MYENV/bin/activate
pip install --no-index --upgrade pip

pip install $HOME/docstring_parser-0.15-py3-none-any.whl --no-index
pip install $HOME/shtab-1.6.4-py3-none-any.whl --no-index
pip install $HOME/tyro-0.5.10-py3-none-any.whl --no-index
pip install $HOME/trl-0.7.2-py3-none-any.whl --no-index


pip install peft --no-index
pip install scikit-learn bitsandbytes sentencepiece protobuf tqdm nltk --no-index
pip install huggingface_hub --no-index
pip install datasets --no-index

pip install xformers accelerate --no-index
pip install seaborn --no-index
pip install transformers --no-index
pip install torch --no-index
pip install h5py --no-index

nvidia-smi

# Execution of the script: replace by python path_to_your_script
# Do not forget to change the pathes to absolute pathes (dont hesitate to use $USER variable)
dataset_choice="eclipse_72k"
id_name=debug
qlora_alpha=16
qlora_r=2
qlora_dropout=0.1
algorithm=finetune_classification
model_name=meta-llama/Llama-2-7b-chat-hf 
new_model_name=Ro128/Llama-2-7b-chat-hf-$id_name-finetuning-gen
lr=0.00001
n_tokens_infered_max=200
cd /project/def-aloise/$USER/

python /project/def-aloise/$USER/main.py -dataset_choice $dataset_choice -path_data_folder /project/def-aloise/$USER/data/ -algorithm $algorithm -id $id_name -new_model_name $new_model_name -qlora_alpha $qlora_alpha -qlora_r $qlora_r -qlora_dropout $qlora_dropout -model_name $model_name -n_tokens_infered_max $n_tokens_infered_max -num_train_epochs 1 -lr $lr


# Capture the end time
end_time=$(date +"%Y-%m-%d %H:%M:%S")

# Calculate and display the execution time
start_seconds=$(date -d "$start_time" '+%s')
end_seconds=$(date -d "$end_time" '+%s')
execution_time=$((end_seconds - start_seconds))

echo "Execution time: $execution_time seconds"