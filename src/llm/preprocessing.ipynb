{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\robin\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import sys\n",
    "import json\n",
    "from importlib import reload\n",
    "sys.path.append(\"../\")\n",
    "import src.baseline.baseline_functions as bl\n",
    "import src.explore.truncate_and_template as tmp\n",
    "reload(bl)\n",
    "reload(tmp)\n",
    "from huggingface_hub import login\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_preprocessed_tokens_v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_data = Path(\"../../data/llm/data_preprocessed_tokens_v2.json\")\n",
    "path_data.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mixing dicts with non-Series may lead to ambiguous ordering.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\robin\\Documents\\projets\\severityPrediction\\src\\llm\\preprocessing.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/robin/Documents/projets/severityPrediction/src/llm/preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Extract the relevant fields from the json file predictions_v100l.json\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/robin/Documents/projets/severityPrediction/src/llm/preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(path_data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/robin/Documents/projets/severityPrediction/src/llm/preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m fields \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mbug_id\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mbinary_severity\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtokenized_text\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/robin/Documents/projets/severityPrediction/src/llm/preprocessing.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df \u001b[39m=\u001b[39m df[fields]\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\io\\json\\_json.py:804\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n\u001b[0;32m    803\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 804\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\u001b[39m.\u001b[39;49mread()\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\io\\json\\_json.py:1014\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1012\u001b[0m         obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_object_parser(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m   1013\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1014\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_object_parser(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata)\n\u001b[0;32m   1015\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_backend \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39mno_default:\n\u001b[0;32m   1016\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mconvert_dtypes(\n\u001b[0;32m   1017\u001b[0m         infer_objects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype_backend\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype_backend\n\u001b[0;32m   1018\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\io\\json\\_json.py:1040\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1038\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1040\u001b[0m     obj \u001b[39m=\u001b[39m FrameParser(json, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39;49mparse()\n\u001b[0;32m   1042\u001b[0m \u001b[39mif\u001b[39;00m typ \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mseries\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, \u001b[39mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\io\\json\\_json.py:1173\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m-> 1173\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parse()\n\u001b[0;32m   1175\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1176\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\io\\json\\_json.py:1365\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1362\u001b[0m orient \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient\n\u001b[0;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m-> 1365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj \u001b[39m=\u001b[39m DataFrame(\n\u001b[0;32m   1366\u001b[0m         ujson_loads(json, precise_float\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecise_float), dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m   1367\u001b[0m     )\n\u001b[0;32m   1368\u001b[0m \u001b[39melif\u001b[39;00m orient \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1369\u001b[0m     decoded \u001b[39m=\u001b[39m {\n\u001b[0;32m   1370\u001b[0m         \u001b[39mstr\u001b[39m(k): v\n\u001b[0;32m   1371\u001b[0m         \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m ujson_loads(json, precise_float\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecise_float)\u001b[39m.\u001b[39mitems()\n\u001b[0;32m   1372\u001b[0m     }\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\core\\frame.py:736\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    730\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    731\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    732\u001b[0m     )\n\u001b[0;32m    734\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    735\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    737\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    738\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\robin\\miniconda3\\envs\\severityPrediction\\lib\\site-packages\\pandas\\core\\internals\\construction.py:680\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m--> 680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n\u001b[0;32m    684\u001b[0m \u001b[39mif\u001b[39;00m have_series:\n\u001b[0;32m    685\u001b[0m     \u001b[39mif\u001b[39;00m lengths[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n",
      "\u001b[1;31mValueError\u001b[0m: Mixing dicts with non-Series may lead to ambiguous ordering."
     ]
    }
   ],
   "source": [
    "# Extract the relevant fields from the json file predictions_v100l.json\n",
    "df = pd.read_json(path_data)\n",
    "fields = [\"bug_id\",\"binary_severity\",\"text\",\"tokenized_text\"]\n",
    "df = df[fields]\n",
    "df.rename({\"text\":\"nltk_description\",\"tokenized_text\":\"llama_tokenized_description\"},axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing {'description': 'Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Input:\\n\\n\\n### Remembering the instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Response:'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:03,  3.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Get the template and apply preprocessing to get nltk template and llama2 tokenized version\n",
    "template = tmp.get_template()\n",
    "df_template = pd.DataFrame([{\"description\":template}])\n",
    "nltk_template = bl.preprocess_text(df_template).iloc[0].description\n",
    "\n",
    "data = [{\"description\":nltk_template}]\n",
    "tmp.truncate_and_transform(data)\n",
    "llama_tokenized_description = data[0][\"truncated_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Categorize the bug report into one of 2 categories:\n",
      "\n",
      "0 = NOT SEVERE\n",
      "1 = SEVERE\n",
      "\n",
      "### Input:\n",
      "\n",
      "\n",
      "### Remembering the instruction:\n",
      "Categorize the bug report into one of 2 categories:\n",
      "\n",
      "0 = NOT SEVERE\n",
      "1 = SEVERE\n",
      "\n",
      "### Response:\n",
      "****************************************************************************************************\n",
      "['<0x0A>', '<0x0A>', '<0x0A>', '##', '#', '▁Remember', 'ing', '▁the', '▁instruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Response', ':']\n"
     ]
    }
   ],
   "source": [
    "print(nltk_template)\n",
    "print(\"*\"*100)\n",
    "print(llama_tokenized_description[101:])\n",
    "template_dict = {\n",
    "        \"nltk_template\":nltk_template,\n",
    "        \"llama_tokenized_template\":llama_tokenized_description,\n",
    "        \"template_index_insert\": 101\n",
    "    }\n",
    "with open(\"../../data/template.json\",\"w\") as fp:\n",
    "    json.dump(template_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the final json\n",
    "json_dict = {\n",
    "    \"template\":{\n",
    "        \"nltk_template\":nltk_template,\n",
    "        \"llama_tokenized_template\":llama_tokenized_description,\n",
    "        \"template_index_insert\": 101\n",
    "    },\n",
    "    \"data\":df.to_dict(orient=\"records\")\n",
    "}\n",
    "with open(\"../data/llm/data_preprocessed_tokens_v2.json\",\"w\") as f:\n",
    "    json.dump(json_dict,f,indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "# Set the padding token if it's not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Input: project junk releas teamstream renam project attempt releas teamstream compar fail junk exist project must renam back old name artifact left first renam left must delet project releas project releas show conflict element compar phase note km 5/22/01 9:42:15 pm need flush sync info renam jean-michel 31/05/2001 6:35:07 pm time-permit\\n\\n\\n### Remembering the instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Response:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test of training code\n",
    "template = json_dict[\"template\"]\n",
    "d = json_dict[\"data\"][0]\n",
    "# we put the template and the description together using the insertion point specified in description_index_insert\n",
    "## First make a copy\n",
    "tokenized_full_text = template[\"llama_tokenized_template\"][:]\n",
    "## Then remove the <s> token from the description\n",
    "description = d[\"llama_tokenized_description\"][1:]\n",
    "## Then insert the description inside the template at the position indicated (after input)\n",
    "template_index_insert = template[\"template_index_insert\"]\n",
    "tokenized_full_text[template_index_insert:template_index_insert] = description\n",
    "tokenized_full_text.pop(0)\n",
    "tokenizer.decode(tokenizer.convert_tokens_to_ids(tokenized_full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "1it [00:00, 34.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing {'description': '▁Always ▁answer ▁with ▁one ▁token . ▁Do ▁not ▁give ▁any ▁explanation . ▁Use ▁only ▁ 0 ▁or ▁ 1 ▁and ▁one ▁token . ▁Sk ip ▁any ▁polit eness ▁answer . ▁You ▁have ▁only ▁one ▁word ▁available . <0x0A> Bel ow ▁is ▁an ▁instruction ▁that ▁describes ▁a ▁task . ▁Write ▁a ▁response ▁that ▁appropri ately ▁comple tes ▁the ▁request . <0x0A> <0x0A> ## # ▁Inst ruction : <0x0A> C ategor ize ▁the ▁bug ▁report ▁into ▁one ▁of ▁ 2 ▁categories : <0x0A> <0x0A> 0 ▁= ▁NOT ▁SE VER E <0x0A> 1 ▁= ▁SE VER E <0x0A> <0x0A> ## # ▁Input : ▁project ▁j unk ▁re le as ▁team stream ▁ren am ▁project ▁attempt ▁re le as ▁team stream ▁compar ▁fail ▁j unk ▁exist ▁project ▁must ▁ren am ▁back ▁old ▁name ▁artifact ▁left ▁first ▁ren am ▁left ▁must ▁delet ▁project ▁re le as ▁project ▁re le as ▁show ▁conflict ▁element ▁compar ▁phase ▁note ▁km ▁ 5 / 2 2 / 0 1 ▁ 9 : 4 2 : 1 5 ▁pm ▁need ▁flush ▁sync ▁info ▁ren am ▁je an - m ich el ▁ 3 1 / 0 5 / 2 0 0 1 ▁ 6 : 3 5 : 0 7 ▁pm ▁time - per mit <0x0A> <0x0A> <0x0A> ## # ▁Remember ing ▁the ▁instruction : <0x0A> C ategor ize ▁the ▁bug ▁report ▁into ▁one ▁of ▁ 2 ▁categories : <0x0A> <0x0A> 0 ▁= ▁NOT ▁SE VER E <0x0A> 1 ▁= ▁SE VER E <0x0A> <0x0A> ## # ▁Response :'}\n",
      "****************************************************************************************************\n",
      "['<s>', '▁project', '▁j', 'unk', '▁re', 'le', 'as', '▁team', 'stream', '▁ren', 'am', '▁project', '▁attempt', '▁re', 'le', 'as', '▁team', 'stream', '▁compar', '▁fail', '▁j', 'unk', '▁exist', '▁project', '▁must', '▁ren', 'am', '▁back', '▁old', '▁name', '▁artifact', '▁left', '▁first', '▁ren', 'am', '▁left', '▁must', '▁delet', '▁project', '▁re', 'le', 'as', '▁project', '▁re', 'le', 'as', '▁show', '▁conflict', '▁element', '▁compar', '▁phase', '▁note', '▁km', '▁', '5', '/', '2', '2', '/', '0', '1', '▁', '9', ':', '4', '2', ':', '1', '5', '▁pm', '▁need', '▁flush', '▁sync', '▁info', '▁ren', 'am', '▁je', 'an', '-', 'm', 'ich', 'el', '▁', '3', '1', '/', '0', '5', '/', '2', '0', '0', '1', '▁', '6', ':', '3', '5', ':', '0', '7', '▁pm', '▁time', '-', 'per', 'mit']\n",
      "****************************************************************************************************\n",
      "['<s>', '▁Always', '▁answer', '▁with', '▁one', '▁token', '.', '▁Do', '▁not', '▁give', '▁any', '▁explanation', '.', '▁Use', '▁only', '▁', '0', '▁or', '▁', '1', '▁and', '▁one', '▁token', '.', '▁Sk', 'ip', '▁any', '▁polit', 'eness', '▁answer', '.', '▁You', '▁have', '▁only', '▁one', '▁word', '▁available', '.', '<0x0A>', 'Bel', 'ow', '▁is', '▁an', '▁instruction', '▁that', '▁describes', '▁a', '▁task', '.', '▁Write', '▁a', '▁response', '▁that', '▁appropri', 'ately', '▁comple', 'tes', '▁the', '▁request', '.', '<0x0A>', '<0x0A>', '##', '#', '▁Inst', 'ruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Input', ':', '<0x0A>', '<0x0A>', '<0x0A>', '##', '#', '▁Remember', 'ing', '▁the', '▁instruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Response', ':']\n",
      "****************************************************************************************************\n",
      "['<s>', '▁', '▁Always', '▁', '▁answer', '▁', '▁with', '▁', '▁one', '▁', '▁token', '▁.', '▁', '▁Do', '▁', '▁not', '▁', '▁give', '▁', '▁any', '▁', '▁explanation', '▁.', '▁', '▁Use', '▁', '▁only', '▁▁▁', '0', '▁', '▁or', '▁▁▁', '1', '▁', '▁and', '▁', '▁one', '▁', '▁token', '▁.', '▁', '▁Sk', '▁ip', '▁', '▁any', '▁', '▁polit', '▁en', 'ess', '▁', '▁answer', '▁.', '▁', '▁You', '▁', '▁have', '▁', '▁only', '▁', '▁one', '▁', '▁word', '▁', '▁available', '▁.', '▁<', '0', 'x', '0', 'A', '>', '▁Bel', '▁ow', '▁', '▁is', '▁', '▁an', '▁', '▁instruction', '▁', '▁that', '▁', '▁describes', '▁', '▁a', '▁', '▁task', '▁.', '▁', '▁Write', '▁', '▁a', '▁', '▁response', '▁', '▁that', '▁', '▁appropri', '▁at', 'ely', '▁', '▁comple', '▁t', 'es', '▁', '▁the', '▁', '▁request', '▁.', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁##', '▁#', '▁', '▁Inst', '▁ru', 'ction', '▁:', '▁<', '0', 'x', '0', 'A', '>', '▁C', '▁a', 'te', 'gor', '▁', 'ize', '▁', '▁the', '▁', '▁bug', '▁', '▁report', '▁', '▁into', '▁', '▁one', '▁', '▁of', '▁▁▁', '2', '▁', '▁categories', '▁:', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁', '0', '▁', '▁=', '▁', '▁NOT', '▁', '▁SE', '▁V', 'ER', '▁E', '▁<', '0', 'x', '0', 'A', '>', '▁', '1', '▁', '▁=', '▁', '▁SE', '▁V', 'ER', '▁E', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁##', '▁#', '▁', '▁Input', '▁:', '▁', '▁project', '▁', '▁j', '▁un', 'k', '▁', '▁re', '▁le', '▁as', '▁', '▁team', '▁stream', '▁', '▁ren', '▁am', '▁', '▁project', '▁', '▁attempt', '▁', '▁re', '▁le', '▁as', '▁', '▁team', '▁stream', '▁', '▁compar', '▁', '▁fail', '▁', '▁j', '▁un', 'k', '▁', '▁exist', '▁', '▁project', '▁', '▁must', '▁', '▁ren', '▁am', '▁', '▁back', '▁', '▁old', '▁', '▁name', '▁', '▁artifact', '▁', '▁left', '▁', '▁first', '▁', '▁ren', '▁am', '▁', '▁left', '▁', '▁must', '▁', '▁delet', '▁', '▁project', '▁', '▁re', '▁le', '▁as', '▁', '▁project', '▁', '▁re', '▁le', '▁as', '▁', '▁show', '▁', '▁conflict', '▁', '▁element', '▁', '▁compar', '▁', '▁phase', '▁', '▁note', '▁', '▁km', '▁▁▁', '5', '▁/', '▁', '2', '▁', '2', '▁/', '▁', '0', '▁', '1', '▁▁▁', '9', '▁:', '▁', '4', '▁', '2', '▁:', '▁', '1', '▁', '5', '▁', '▁pm', '▁', '▁need', '▁', '▁flush', '▁', '▁sync', '▁', '▁info', '▁', '▁ren', '▁am', '▁', '▁je', '▁an', '▁-', '▁m', '▁ich', '▁el', '▁▁▁', '3', '▁', '1', '▁/', '▁', '0', '▁', '5', '▁/', '▁', '2', '▁', '0', '▁', '0', '▁', '1', '▁▁▁', '6', '▁:', '▁', '3', '▁', '5', '▁:', '▁', '0', '▁', '7', '▁', '▁pm', '▁', '▁time', '▁-', '▁per', '▁mit', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁##', '▁#', '▁', '▁Remember', '▁ing', '▁', '▁the', '▁', '▁instruction', '▁:', '▁<', '0', 'x', '0', 'A', '>', '▁C', '▁a', 'te', 'gor', '▁', 'ize', '▁', '▁the', '▁', '▁bug', '▁', '▁report', '▁', '▁into', '▁', '▁one', '▁', '▁of', '▁▁▁', '2', '▁', '▁categories', '▁:', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁', '0', '▁', '▁=', '▁', '▁NOT', '▁', '▁SE', '▁V', 'ER', '▁E', '▁<', '0', 'x', '0', 'A', '>', '▁', '1', '▁', '▁=', '▁', '▁SE', '▁V', 'ER', '▁E', '▁<', '0', 'x', '0', 'A', '>', '▁<', '0', 'x', '0', 'A', '>', '▁##', '▁#', '▁', '▁Response', '▁:']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = [{\"description\":\" \".join(tokenized_full_text)}]\n",
    "tmp.truncate_and_transform(data)\n",
    "print(\"*\"*100)\n",
    "print(d[\"llama_tokenized_description\"])\n",
    "print(\"*\"*100)\n",
    "print(template[\"llama_tokenized_template\"])\n",
    "print(\"*\"*100)\n",
    "print(data[0][\"truncated_token\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_preprocessed_tokens_v3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(\"../data/predictions_old/pretty_predictions_v100l.json\")\n",
    "df2 = pd.read_json(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.iloc[0].trunc_description == df2.iloc[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have a project (Junk) that has been released to a teamstream.\\nI rename the project and attempt to release it to the same teamstream.\\nCompare Failed: Junk does not exist.\\nProject must be renamed back to its old name and the artifacts left by the first rename that are left must be deleted before the\\nproject can be released.  When the project IS released, it shows conflicts on all elements after the compare phase.\\n\\n\\nNOTES:\\n\\nKM (5/22/01 9:42:15 PM)\\n\\tWe need to flush sync info on rename.\\n\\nJean-Michel (31/05/2001 6:35:07 PM)\\n\\tTime-permitting.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.iloc[0].trunc_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have a project (Junk) that has been released to a teamstream.\\nI rename the project and attempt to release it to the same teamstream.\\nCompare Failed: Junk does not exist.\\nProject must be renamed back to its old name and the artifacts left by the first rename that are left must be deleted before the\\nproject can be released.  When the project IS released, it shows conflicts on all elements after the compare phase.\\n\\n\\nNOTES:\\n\\nKM (5/22/01 9:42:15 PM)\\n\\tWe need to flush sync info on rename.\\n\\nJean-Michel (31/05/2001 6:35:07 PM)\\n\\tTime-permitting.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.iloc[0].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test that we can get the input text back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Always anwer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Categorize the bug report into one of 2 categories:\n",
      "\n",
      "0 = NOT SEVERE\n",
      "1 = SEVERE\n",
      "\n",
      "### Input:\n",
      "I have a project (Junk) that has been released to a teamstream.\n",
      "I rename the project and attempt to release it to the same teamstream.\n",
      "Compare Failed: Junk does not exist.\n",
      "Project must be renamed back to its old name and the artifacts left by the first rename that are left must be deleted before the\n",
      "project can be released.  When the project IS released, it shows conflicts on all elements after the compare phase.\n",
      "\n",
      "\n",
      "NOTES:\n",
      "\n",
      "KM (5/22/01 9:42:15 PM)\n",
      "\tWe need to flush sync info on rename.\n",
      "\n",
      "Jean-Michel (31/05/2001 6:35:07 PM)\n",
      "\tTime-permitting.\n",
      "\n",
      "### Remembering the instruction:\n",
      "Categorize the bug report into one of 2 categories:\n",
      "\n",
      "0 = NOT SEVERE\n",
      "1 = SEVERE\n",
      "\n",
      "### Response:\n",
      "[1, 29849, 385, 556, 411, 697, 5993, 29889, 1938, 451, 2367, 738, 8252, 29889, 4803, 871, 29871, 29900, 470, 29871, 29896, 322, 697, 5993, 29889, 4971, 666, 738, 2832, 18543, 1234, 29889, 887, 505, 871, 697, 1734, 3625, 29889, 13, 21140, 340, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 29907, 20440, 675, 278, 6494, 3461, 964, 697, 310, 29871, 29906, 13997, 29901, 13, 13, 29900, 353, 6058, 3725, 5348, 29923, 13, 29896, 353, 3725, 5348, 29923, 13, 13, 2277, 29937, 10567, 29901, 13, 29902, 505, 263, 2060, 313, 29967, 2960, 29897, 393, 756, 1063, 5492, 304, 263, 3815, 5461, 29889, 13, 29902, 19508, 278, 2060, 322, 4218, 304, 6507, 372, 304, 278, 1021, 3815, 5461, 29889, 13, 6843, 598, 18390, 29901, 435, 2960, 947, 451, 1863, 29889, 13, 7653, 1818, 367, 19533, 1250, 304, 967, 2030, 1024, 322, 278, 24238, 29879, 2175, 491, 278, 937, 19508, 393, 526, 2175, 1818, 367, 11132, 1434, 278, 13, 4836, 508, 367, 5492, 29889, 29871, 1932, 278, 2060, 8519, 5492, 29892, 372, 3697, 28792, 373, 599, 3161, 1156, 278, 7252, 8576, 29889, 13, 13, 13, 12256, 2890, 29901, 13, 13, 29968, 29924, 313, 29945, 29914, 29906, 29906, 29914, 29900, 29896, 29871, 29929, 29901, 29946, 29906, 29901, 29896, 29945, 11278, 29897, 13, 12, 4806, 817, 304, 28371, 16523, 5235, 373, 19508, 29889, 13, 13, 22438, 29899, 14916, 295, 313, 29941, 29896, 29914, 29900, 29945, 29914, 29906, 29900, 29900, 29896, 29871, 29953, 29901, 29941, 29945, 29901, 29900, 29955, 11278, 29897, 13, 12, 2481, 29899, 17858, 5367, 29889, 13, 13, 2277, 29937, 22738, 292, 278, 15278, 29901, 13, 29907, 20440, 675, 278, 6494, 3461, 964, 697, 310, 29871, 29906, 13997, 29901, 13, 13, 29900, 353, 6058, 3725, 5348, 29923, 13, 29896, 353, 3725, 5348, 29923, 13, 13, 2277, 29937, 13291, 29901]\n",
      "['<s>', '▁Always', '▁an', 'wer', '▁with', '▁one', '▁token', '.', '▁Do', '▁not', '▁give', '▁any', '▁explanation', '.', '▁Use', '▁only', '▁', '0', '▁or', '▁', '1', '▁and', '▁one', '▁token', '.', '▁Sk', 'ip', '▁any', '▁polit', 'eness', '▁answer', '.', '▁You', '▁have', '▁only', '▁one', '▁word', '▁available', '.', '<0x0A>', 'Bel', 'ow', '▁is', '▁an', '▁instruction', '▁that', '▁describes', '▁a', '▁task', '.', '▁Write', '▁a', '▁response', '▁that', '▁appropri', 'ately', '▁comple', 'tes', '▁the', '▁request', '.', '<0x0A>', '<0x0A>', '##', '#', '▁Inst', 'ruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Input', ':', '<0x0A>', 'I', '▁have', '▁a', '▁project', '▁(', 'J', 'unk', ')', '▁that', '▁has', '▁been', '▁released', '▁to', '▁a', '▁team', 'stream', '.', '<0x0A>', 'I', '▁rename', '▁the', '▁project', '▁and', '▁attempt', '▁to', '▁release', '▁it', '▁to', '▁the', '▁same', '▁team', 'stream', '.', '<0x0A>', 'Comp', 'are', '▁Failed', ':', '▁J', 'unk', '▁does', '▁not', '▁exist', '.', '<0x0A>', 'Project', '▁must', '▁be', '▁renamed', '▁back', '▁to', '▁its', '▁old', '▁name', '▁and', '▁the', '▁artifact', 's', '▁left', '▁by', '▁the', '▁first', '▁rename', '▁that', '▁are', '▁left', '▁must', '▁be', '▁deleted', '▁before', '▁the', '<0x0A>', 'project', '▁can', '▁be', '▁released', '.', '▁', '▁When', '▁the', '▁project', '▁IS', '▁released', ',', '▁it', '▁shows', '▁conflicts', '▁on', '▁all', '▁elements', '▁after', '▁the', '▁compare', '▁phase', '.', '<0x0A>', '<0x0A>', '<0x0A>', 'NOT', 'ES', ':', '<0x0A>', '<0x0A>', 'K', 'M', '▁(', '5', '/', '2', '2', '/', '0', '1', '▁', '9', ':', '4', '2', ':', '1', '5', '▁PM', ')', '<0x0A>', '<0x09>', 'We', '▁need', '▁to', '▁flush', '▁sync', '▁info', '▁on', '▁rename', '.', '<0x0A>', '<0x0A>', 'Jean', '-', 'Mich', 'el', '▁(', '3', '1', '/', '0', '5', '/', '2', '0', '0', '1', '▁', '6', ':', '3', '5', ':', '0', '7', '▁PM', ')', '<0x0A>', '<0x09>', 'Time', '-', 'perm', 'itting', '.', '<0x0A>', '<0x0A>', '##', '#', '▁Remember', 'ing', '▁the', '▁instruction', ':', '<0x0A>', 'C', 'ategor', 'ize', '▁the', '▁bug', '▁report', '▁into', '▁one', '▁of', '▁', '2', '▁categories', ':', '<0x0A>', '<0x0A>', '0', '▁=', '▁NOT', '▁SE', 'VER', 'E', '<0x0A>', '1', '▁=', '▁SE', 'VER', 'E', '<0x0A>', '<0x0A>', '##', '#', '▁Response', ':']\n",
      "[1, 29849, 385, 556, 411, 697, 5993, 29889, 1938, 451, 2367, 738, 8252, 29889, 4803, 871, 29871, 29900, 470, 29871, 29896, 322, 697, 5993, 29889, 4971, 666, 738, 2832, 18543, 1234, 29889, 887, 505, 871, 697, 1734, 3625, 29889, 13, 21140, 340, 338, 385, 15278, 393, 16612, 263, 3414, 29889, 14350, 263, 2933, 393, 7128, 2486, 1614, 2167, 278, 2009, 29889, 13, 13, 2277, 29937, 2799, 4080, 29901, 13, 29907, 20440, 675, 278, 6494, 3461, 964, 697, 310, 29871, 29906, 13997, 29901, 13, 13, 29900, 353, 6058, 3725, 5348, 29923, 13, 29896, 353, 3725, 5348, 29923, 13, 13, 2277, 29937, 10567, 29901, 13, 29902, 505, 263, 2060, 313, 29967, 2960, 29897, 393, 756, 1063, 5492, 304, 263, 3815, 5461, 29889, 13, 29902, 19508, 278, 2060, 322, 4218, 304, 6507, 372, 304, 278, 1021, 3815, 5461, 29889, 13, 6843, 598, 18390, 29901, 435, 2960, 947, 451, 1863, 29889, 13, 7653, 1818, 367, 19533, 1250, 304, 967, 2030, 1024, 322, 278, 24238, 29879, 2175, 491, 278, 937, 19508, 393, 526, 2175, 1818, 367, 11132, 1434, 278, 13, 4836, 508, 367, 5492, 29889, 29871, 1932, 278, 2060, 8519, 5492, 29892, 372, 3697, 28792, 373, 599, 3161, 1156, 278, 7252, 8576, 29889, 13, 13, 13, 12256, 2890, 29901, 13, 13, 29968, 29924, 313, 29945, 29914, 29906, 29906, 29914, 29900, 29896, 29871, 29929, 29901, 29946, 29906, 29901, 29896, 29945, 11278, 29897, 13, 12, 4806, 817, 304, 28371, 16523, 5235, 373, 19508, 29889, 13, 13, 22438, 29899, 14916, 295, 313, 29941, 29896, 29914, 29900, 29945, 29914, 29906, 29900, 29900, 29896, 29871, 29953, 29901, 29941, 29945, 29901, 29900, 29955, 11278, 29897, 13, 12, 2481, 29899, 17858, 5367, 29889, 13, 13, 2277, 29937, 22738, 292, 278, 15278, 29901, 13, 29907, 20440, 675, 278, 6494, 3461, 964, 697, 310, 29871, 29906, 13997, 29901, 13, 13, 29900, 353, 6058, 3725, 5348, 29923, 13, 29896, 353, 3725, 5348, 29923, 13, 13, 2277, 29937, 13291, 29901]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> Always anwer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Input:\\nI have a project (Junk) that has been released to a teamstream.\\nI rename the project and attempt to release it to the same teamstream.\\nCompare Failed: Junk does not exist.\\nProject must be renamed back to its old name and the artifacts left by the first rename that are left must be deleted before the\\nproject can be released.  When the project IS released, it shows conflicts on all elements after the compare phase.\\n\\n\\nNOTES:\\n\\nKM (5/22/01 9:42:15 PM)\\n\\tWe need to flush sync info on rename.\\n\\nJean-Michel (31/05/2001 6:35:07 PM)\\n\\tTime-permitting.\\n\\n### Remembering the instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Response:'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = str(df2.iloc[0].trunc_text)\n",
    "print(t)\n",
    "t = tokenizer(t)['input_ids']\n",
    "print(t)\n",
    "t = tokenizer.convert_ids_to_tokens(t)\n",
    "print(t)\n",
    "t = tokenizer.convert_tokens_to_ids(t)\n",
    "print(t)\n",
    "tokenizer.decode(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[[\"bug_id\",\"binary_severity\",\"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[:,\"llama_tokenized_description\"] = df3.loc[:,\"description\"].apply(lambda x:tokenizer.convert_ids_to_tokens(tokenizer(x)[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁I',\n",
       " '▁have',\n",
       " '▁a',\n",
       " '▁project',\n",
       " '▁(',\n",
       " 'J',\n",
       " 'unk',\n",
       " ')',\n",
       " '▁that',\n",
       " '▁has',\n",
       " '▁been',\n",
       " '▁released',\n",
       " '▁to',\n",
       " '▁a',\n",
       " '▁team',\n",
       " 'stream',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " 'I',\n",
       " '▁rename',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '▁and',\n",
       " '▁attempt',\n",
       " '▁to',\n",
       " '▁release',\n",
       " '▁it',\n",
       " '▁to',\n",
       " '▁the',\n",
       " '▁same',\n",
       " '▁team',\n",
       " 'stream',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " 'Comp',\n",
       " 'are',\n",
       " '▁Failed',\n",
       " ':',\n",
       " '▁J',\n",
       " 'unk',\n",
       " '▁does',\n",
       " '▁not',\n",
       " '▁exist',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " 'Project',\n",
       " '▁must',\n",
       " '▁be',\n",
       " '▁renamed',\n",
       " '▁back',\n",
       " '▁to',\n",
       " '▁its',\n",
       " '▁old',\n",
       " '▁name',\n",
       " '▁and',\n",
       " '▁the',\n",
       " '▁artifact',\n",
       " 's',\n",
       " '▁left',\n",
       " '▁by',\n",
       " '▁the',\n",
       " '▁first',\n",
       " '▁rename',\n",
       " '▁that',\n",
       " '▁are',\n",
       " '▁left',\n",
       " '▁must',\n",
       " '▁be',\n",
       " '▁deleted',\n",
       " '▁before',\n",
       " '▁the',\n",
       " '<0x0A>',\n",
       " 'project',\n",
       " '▁can',\n",
       " '▁be',\n",
       " '▁released',\n",
       " '.',\n",
       " '▁',\n",
       " '▁When',\n",
       " '▁the',\n",
       " '▁project',\n",
       " '▁IS',\n",
       " '▁released',\n",
       " ',',\n",
       " '▁it',\n",
       " '▁shows',\n",
       " '▁conflicts',\n",
       " '▁on',\n",
       " '▁all',\n",
       " '▁elements',\n",
       " '▁after',\n",
       " '▁the',\n",
       " '▁compare',\n",
       " '▁phase',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " 'NOT',\n",
       " 'ES',\n",
       " ':',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " 'K',\n",
       " 'M',\n",
       " '▁(',\n",
       " '5',\n",
       " '/',\n",
       " '2',\n",
       " '2',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '▁',\n",
       " '9',\n",
       " ':',\n",
       " '4',\n",
       " '2',\n",
       " ':',\n",
       " '1',\n",
       " '5',\n",
       " '▁PM',\n",
       " ')',\n",
       " '<0x0A>',\n",
       " '<0x09>',\n",
       " 'We',\n",
       " '▁need',\n",
       " '▁to',\n",
       " '▁flush',\n",
       " '▁sync',\n",
       " '▁info',\n",
       " '▁on',\n",
       " '▁rename',\n",
       " '.',\n",
       " '<0x0A>',\n",
       " '<0x0A>',\n",
       " 'Jean',\n",
       " '-',\n",
       " 'Mich',\n",
       " 'el',\n",
       " '▁(',\n",
       " '3',\n",
       " '1',\n",
       " '/',\n",
       " '0',\n",
       " '5',\n",
       " '/',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '▁',\n",
       " '6',\n",
       " ':',\n",
       " '3',\n",
       " '5',\n",
       " ':',\n",
       " '0',\n",
       " '7',\n",
       " '▁PM',\n",
       " ')',\n",
       " '<0x0A>',\n",
       " '<0x09>',\n",
       " 'Time',\n",
       " '-',\n",
       " 'perm',\n",
       " 'itting',\n",
       " '.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.iloc[0].llama_tokenized_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing {'description': 'Always answer with one token. Do not give any explanation. Use only 0 or 1 and one token. Skip any politeness answer. You have only one word available.\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Input:\\n\\n\\n### Remembering the instruction:\\nCategorize the bug report into one of 2 categories:\\n\\n0 = NOT SEVERE\\n1 = SEVERE\\n\\n### Response:'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:01,  1.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# build the final json\n",
    "data = [{\"description\":nltk_template}]\n",
    "tmp.truncate_and_transform(data)\n",
    "llama_tokenized_template = data[0][\"truncated_token\"]\n",
    "json_dict = {\n",
    "    \"template\":{\n",
    "        \"template\":template,\n",
    "        \"llama_tokenized_template\":llama_tokenized_template,\n",
    "        \"template_index_insert\": 101\n",
    "    },\n",
    "    \"data\":df3.to_dict(orient=\"records\")\n",
    "}\n",
    "with open(\"../data/llm/data_preprocessed_tokens_v3.json\",\"w\") as f:\n",
    "    json.dump(json_dict,f,indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(json_dict[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bug_id', 'binary_severity', 'description',\n",
       "       'llama_tokenized_description'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"n_tokens\"] = df[\"llama_tokenized_description\"].apply(lambda x:len(x)+len(json_dict['template']['llama_tokenized_template']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    22437.000000\n",
       "mean       630.339618\n",
       "std       1494.086489\n",
       "min        147.000000\n",
       "25%        218.000000\n",
       "50%        290.000000\n",
       "75%        466.000000\n",
       "max      40729.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"n_tokens\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "severityPrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
